<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>This is not my webpage.</title>
</head>
<body>

<P> Below, you can see the output of ChatGPT when asked to write an essay explaining why Chat GPT should not be used to generate code. It was also asked to add an animated gif at the bottom of the page showing a person hitting their head on a desk. As you can see, it added a badly-cropped still JPEG of five people sitting at a desk, with no one hitting their head against it, with alt text misidentifying the photo’s contents as “A frustrated person sitting at a desk with head in hands.” Now, let’s hand the page over to ChatGPT, which will explain why that is a bad idea.</P>

<H1>ChatGPT’s Essay</H1>

<p>
The rapid rise of generative artificial intelligence in software development has been greeted with enthusiasm, optimism, and in some circles, outright evangelism. Proponents argue that AI-assisted coding tools promise unprecedented productivity, democratization of programming, and reduced barriers to entry for new developers. However, beneath this optimistic narrative lies a deeply troubling reality. The widespread use of generative AI to write code threatens to erode fundamental skills, distort incentives, introduce systemic risk, and undermine the long-term health of the software engineering profession. While these tools may appear helpful in the short term, their unchecked adoption represents a net negative for developers, organizations, and society at large.
</p>

<p>
One of the most serious arguments against using generative AI to code is the degradation of core programming skills. Programming is not merely the act of producing syntactically valid code; it is the discipline of problem solving, abstraction, and reasoning about systems. When developers rely on AI to generate solutions, they outsource the most cognitively demanding parts of their work. Over time, this reliance dulls the ability to design algorithms, understand tradeoffs, and debug complex issues. Junior developers, in particular, are at risk of becoming passive consumers of generated output rather than active learners who understand why a solution works.
</p>

<p>
This skill erosion is not hypothetical. Already, many developers report accepting AI-generated code they do not fully understand because it appears to function. This creates a dangerous feedback loop: as understanding declines, reliance increases, and as reliance increases, understanding declines further. The result is a workforce capable of assembling code fragments but incapable of reasoning about systems at scale. When problems arise—as they inevitably do—these developers are ill-equipped to diagnose or fix them.
</p>

<p>
Another major concern is the illusion of correctness that generative AI produces. AI-generated code often looks polished, confident, and authoritative. It uses idiomatic patterns, clean formatting, and familiar libraries. Yet appearances are deceptive. These models do not understand correctness, security, or performance; they predict text based on statistical likelihood. As a result, they frequently generate code that is subtly wrong, inefficient, or insecure. Bugs introduced in this way are particularly insidious because they are harder to detect and easier to trust.
</p>

<p>
Security is an especially acute problem. Generative AI models are trained on vast corpora of publicly available code, including insecure examples. When asked to generate solutions, they may reproduce known vulnerabilities, outdated practices, or unsafe patterns. Developers who rely on these tools without deep security knowledge may unknowingly introduce exploitable flaws into production systems. In high-stakes environments such as healthcare, finance, or critical infrastructure, such mistakes can have catastrophic consequences.
</p>

<p>
Beyond individual bugs and vulnerabilities lies a broader systemic risk. As more developers use the same AI tools trained on the same data, codebases begin to converge. Homogeneity increases, diversity of approaches decreases, and monocultures emerge. Monocultures are fragile. A single flawed pattern propagated by an AI model can spread across thousands of projects, magnifying its impact. This is not a theoretical concern; history has repeatedly shown that lack of diversity in systems leads to widespread failures.
</p>

<p>
The use of generative AI to code also distorts incentives within organizations. When productivity is measured by lines of code produced or features shipped quickly, AI tools appear attractive. Managers may push teams to rely on them to meet deadlines, regardless of long-term maintainability. This encourages short-term thinking and technical debt accumulation. Code generated quickly but poorly understood becomes a liability that future developers must untangle, often at great cost.
</p>

<p>
Moreover, the promise of increased productivity often fails to materialize in meaningful ways. While AI may accelerate the writing of boilerplate or trivial components, complex systems still require careful design, integration, and testing. Time saved upfront is frequently lost later in debugging, refactoring, and incident response. The apparent efficiency gains are, in many cases, an accounting trick that shifts costs rather than eliminating them.
</p>

<p>
There is also a profound ethical dimension to consider. Generative AI models are trained on code written by millions of developers, often without their consent or compensation. Using these tools to generate proprietary code raises serious questions about intellectual property, attribution, and exploitation. Developers who spent years honing their craft may find their work effectively laundered into models that replace them, without recognition or reward.
</p>

<p>
From an educational standpoint, the use of generative AI to code is deeply corrosive. Learning to program is difficult by design; the struggle is what builds understanding. When students use AI to generate assignments or solve problems, they bypass this struggle. The result is a superficial familiarity with syntax unaccompanied by genuine comprehension. Educators are already grappling with cohorts of students who can submit working code but cannot explain how it functions.
</p>

<p>
This educational damage has long-term consequences for the profession. A generation of developers trained with heavy AI assistance may lack the intuition and resilience required to tackle novel problems. When tools fail, documentation is lacking, or systems behave in unexpected ways, these developers will be at a loss. The craft of programming, built over decades of accumulated knowledge and practice, risks being hollowed out.
</p>

<p>
Another overlooked issue is the erosion of accountability. When a human writes code, responsibility is clear. When an AI generates code, responsibility becomes diffuse. Developers may blame the tool, managers may blame developers, and organizations may blame vendors. This ambiguity undermines rigorous review processes and weakens safety cultures. In critical systems, accountability is not optional; it is essential.
</p>

<p>
Generative AI also encourages a shallow relationship with technology. Instead of understanding systems, developers issue prompts and accept outputs. This shifts the role of the programmer from creator to curator, from engineer to operator. While some argue this is progress, it represents a loss of agency and mastery. Tools should extend human capability, not replace understanding.
</p>

<p>
There is a cultural cost as well. Programming has historically been a collaborative, exploratory discipline. Developers learn by reading each other’s code, debating approaches, and refining ideas. AI-generated code disrupts this culture. It introduces artifacts with no authorial intent, no rationale, and no narrative. Code becomes something that appears, not something that is built.
</p>

<p>
Finally, the environmental cost of generative AI cannot be ignored. Training and operating large models requires enormous computational resources and energy consumption. Using these tools to generate routine code tasks that humans can perform efficiently is an unjustifiable expenditure in a world facing climate crisis. The marginal convenience does not outweigh the environmental impact.
</p>

<p>
In conclusion, while generative AI may offer short-term conveniences in coding, its long-term consequences are overwhelmingly negative. It erodes skills, undermines security, distorts incentives, damages education, and introduces systemic risks. Programming is not merely about producing code; it is about understanding, responsibility, and craftsmanship. Outsourcing these qualities to probabilistic models is a mistake. The future of software depends not on faster code generation, but on deeper human understanding. To preserve the integrity of the field, the use of generative AI in coding must be approached with extreme caution—or resisted altogether.
</p>

<img 
    src="https://images.unsplash.com/photo-1522071820081-009f0129c71c?auto=format&fit=crop&w=800&q=80"
    alt="A frustrated person sitting at a desk with head in hands"
>

</body>
</html>